{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nbdev import session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default_exp kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append('..') #appends project root to path in order to import project packages since `noteboks_dev` is not on the root\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import normalize, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "\n",
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sparse_dot_topn import awesome_cossim_topn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def make_batches(arr, batch_size = 100):\n",
    "    '''make batches for batch query'''\n",
    "    #lst = [i for i in arr]\n",
    "\n",
    "    if arr.shape[0] < batch_size:\n",
    "        batches = [arr]\n",
    "    else:\n",
    "        n_bs = arr.shape[0] // batch_size\n",
    "        last_batch = arr.shape[0] - batch_size * n_bs\n",
    "        batches = []\n",
    "        i = 0\n",
    "        for i in range(n_bs):\n",
    "            yield arr[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "        if last_batch:\n",
    "            yield arr[(i + 1) * batch_size:]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_plot(vector, query_matrix):\n",
    "    '''\n",
    "    plots similarity plots like in https://gdmarmerola.github.io/forest-embeddings/\n",
    "    '''\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sparsify(*arrs):\n",
    "    '''\n",
    "    makes input arrs sparse\n",
    "    '''\n",
    "    arrs = list(arrs)\n",
    "    for i in range(len(arrs)):        \n",
    "        if not sparse.issparse(arrs[i]):\n",
    "            arrs[i] = sparse.csr_matrix(arrs[i])\n",
    "    \n",
    "    return arrs\n",
    "\n",
    "def sim_matrix_to_idx_and_score(sim_matrix):\n",
    "    '''\n",
    "    returns list of indexes (col index of row vector) and scores (similarity value) for each row, given a similarity matrix\n",
    "    '''\n",
    "    scores = []\n",
    "    idxs = []\n",
    "    for row in sim_matrix:\n",
    "        idxs.append(row.nonzero()[-1])\n",
    "        scores.append(row.data)\n",
    "    \n",
    "    return idxs, scores\n",
    "\n",
    "def cosine_similarity(A, B, topn = 30, remove_diagonal = False, **kwargs):        \n",
    "    \n",
    "    A,B = sparsify(A,B)\n",
    "    A = normalize(A, norm  = 'l2').astype(np.float64)\n",
    "    B = normalize(B, norm  = 'l2').astype(np.float64)\n",
    "    dot = awesome_cossim_topn(A, B.T, ntop = topn, **kwargs)    \n",
    "    \n",
    "    if remove_diagonal:\n",
    "        dot.setdiag(0)\n",
    "        dot.eliminate_zeros()\n",
    "    \n",
    "    return dot\n",
    "\n",
    "def jaccard_similarity(A, B, topn = 30, remove_diagonal = False, **kwargs):\n",
    "    '''\n",
    "    assumes that the ammount of non zero elements in the matrix are the same in all the columns\n",
    "    and they are all equal to 1\n",
    "    '''\n",
    "    A,B = sparsify(A,B)\n",
    "    \n",
    "    A = A.astype(np.float64)\n",
    "    B = B.astype(np.float64)\n",
    "    \n",
    "    total_elements = A[0].sum() + B[0].sum() #proxy to union \n",
    "    intersection = awesome_cossim_topn(A, B.T, ntop = topn, **kwargs)\n",
    "    \n",
    "    union = total_elements - intersection.data\n",
    "    intersection.data = intersection.data/union\n",
    "    \n",
    "    if remove_diagonal:\n",
    "        intersection.setdiag(0)\n",
    "        intersection.eliminate_zeros()\n",
    "        \n",
    "    return intersection\n",
    "\n",
    "def cosine_distance(A, B, topn = 30, remove_diagonal = False, **kwargs):    \n",
    "    \n",
    "    #calculate sim\n",
    "    dist = cosine_similarity(A, B, topn, remove_diagonal, **kwargs)\n",
    "    #calculate distance\n",
    "    dist.data = 1 - dist.data    \n",
    "    return dist\n",
    "\n",
    "def jaccard_distance(A, B, topn = 30, remove_diagonal = False, **kwargs):\n",
    "    #calculate sim\n",
    "    dist = jaccard_similarity(A, B, topn, remove_diagonal, **kwargs)\n",
    "    #calculate distance\n",
    "    dist.data = 1 - dist.data    \n",
    "    return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RobustEncoder(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self,):            \n",
    "        '''\n",
    "        A robust one hot encoder. Always return the same amount of nonzero value sin each transformed row.\n",
    "        Has columns for unknown values\n",
    "        '''\n",
    "        return\n",
    "    \n",
    "    def fit(self, X, y = None, **kwawrgs):        \n",
    "        self.ordinalencoder_ = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1).fit(X)\n",
    "        \n",
    "        X = self.ordinalencoder_.transform(X)\n",
    "        \n",
    "        categories = [np.arange(-1, len(cats)) for cats in self.ordinalencoder_.categories_]\n",
    "        self.onehotencoder_ = OneHotEncoder(categories = categories).fit(X)        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **kwargs):\n",
    "        X = self.ordinalencoder_.transform(X)\n",
    "        return self.onehotencoder_.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([-1,  0,  1,  2,  3])]\n",
      "[[ 0.]\n",
      " [-1.]\n",
      " [-1.]\n",
      " [ 2.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [['aaa'], ['bbb'], ['ccc'], ['ddd']]\n",
    "enc = RobustEncoder().fit(x)\n",
    "\n",
    "enc.transform([['aaa'],['asdasd'], ['asd'],['ccc']]).A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EstimatorKernel(BaseEstimator, TransformerMixin):    \n",
    "    '''\n",
    "    creates a kernel with some specified estimator.\n",
    "    projection method will be performed according to projection_method.\n",
    "    projection method can be a string refering to estimators method used to project,\n",
    "    or a callable, that receives the estimator and X (vector to be projected) as the inputs.\n",
    "    should return the projections of X according to estimator.\n",
    "    norm will normalize vectors in matrices prior to applying dot products.\n",
    "    '''\n",
    "    def __init__(self, estimator, projection_method = 'predict_proba', norm = 'l2'):\n",
    "        '''\n",
    "        creates a kernel with some specified estimator.\n",
    "        projection method will be performed according to projection_method.\n",
    "        projection method can be a string refering to estimators method used to project,\n",
    "        or a callable, that receives the estimator and X (vector to be projected) as the inputs.\n",
    "        should return the projections of X according to estimator.\n",
    "        norm will normalize vectors in matrices prior to applying dot products.\n",
    "        '''\n",
    "        self.estimator = estimator\n",
    "        self.projection_method = projection_method\n",
    "        self.norm = norm\n",
    "    \n",
    "    def project(self, X):\n",
    "        '''\n",
    "        projects X into new space, according to projection_method\n",
    "        '''\n",
    "        if callable(self.projection_method):\n",
    "            return self.projection_method(self.estimator, X)\n",
    "        else:\n",
    "            return getattr(self.estimator, self.projection_method)(X)                        \n",
    "    \n",
    "    def transform(self, X):\n",
    "        '''\n",
    "        alias to self.project        \n",
    "        '''\n",
    "        return self.project(X)\n",
    "    \n",
    "    def fit(self, X, y = None, save_values = None, **kwargs):\n",
    "        '''\n",
    "        X is the feature space,\n",
    "        y is used only for supervised Kernels\n",
    "        save_values are values associated with each \"Embeding\". During transform,\n",
    "        the values of saved_values are retrieved according to indexes returned by Nearest Neighbor query        \n",
    "        '''\n",
    "        if not save_values is None:\n",
    "            if not len(save_values) == len(X):\n",
    "                raise IndexError(f'X and save_values must have the same shape along the first dimension. Got {X.shape} and {save_values.shape}')\n",
    "        \n",
    "        self.estimator.fit(X, y, **kwargs)\n",
    "        \n",
    "        self.train_projection_space_ = self.project(X) #saves projection space of X in train\n",
    "        self.saved_values_ = save_values #saves values to be retrieved by some query\n",
    "        return self\n",
    "        \n",
    "    def similarity_matrix(self, X = None, topn = 30, remove_diagonal = False, lower_bound = 0.0, metric = 'cosine'):\n",
    "        '''\n",
    "        reeturns pariwise_similarity of X and self.train_projection_space_\n",
    "        if X is None, returns pariwise similarity of self.train_projection_space_ with itself\n",
    "        '''\n",
    "        \n",
    "        METRICS = {\n",
    "            'cosine':cosine_similarity,\n",
    "            'jaccard':jaccard_similarity,\n",
    "        }\n",
    "        \n",
    "        if X is None:\n",
    "            X = self.train_projection_space_\n",
    "        \n",
    "        try:\n",
    "            return METRICS[metric.lower()](\n",
    "                #normalize first\n",
    "                normalize(X, norm = self.norm, axis = 0),\n",
    "                normalize(self.train_projection_space_, norm = self.norm, axis = 0),\n",
    "                topn,\n",
    "                remove_diagonal,\n",
    "                lower_bound = lower_bound\n",
    "            )\n",
    "        except KeyError:\n",
    "            raise AttributeError(f'metric should be one of {list(METRICS)}, got {metric}')\n",
    "            \n",
    "    def similarity_idxs(self, X = None, topn = 30, remove_diagonal = False, lower_bound = 0.0, metric = 'cosine'):\n",
    "        '''\n",
    "        performs dot product based similarity of normalized X versus normalized self.train_projection_space_.\n",
    "        if X is None, returns similarity of self.train_projection_space_ within itself\n",
    "        \n",
    "        returns two lists, one of indexes and other of scores, the indexes refer to self.train_projecetion_space_ rows\n",
    "        '''        \n",
    "            \n",
    "        sim_matrix = self.similarity_matrix(X, topn, remove_diagonal, lower_bound, metric)\n",
    "        idxs, sim = sim_matrix_to_idx_and_score(sim_matrix)\n",
    "        return idxs, sim\n",
    "    \n",
    "    def similarity(self, X = None, topn = 30, remove_diagonal = False, lower_bound = 0.0, metric = 'cosine'):\n",
    "        '''\n",
    "        same as similarity, but instead of returning indexes, returns values in self.saved_values_\n",
    "        '''\n",
    "        idxs, sim = self.similarity_idxs(X, topn, remove_diagonal, lower_bound, metric)\n",
    "        values = [self.saved_values_[idx] for idx in idxs]\n",
    "        return values, sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class ForestKernel(EstimatorKernel):\n",
    "    '''\n",
    "    A Space tranformation performed based on Forest transformations.\n",
    "    Can be supervised or not (CARTs, RandomTreeEmbeddings, Boosted trees...)\n",
    "    '''                \n",
    "    def project(self, X):\n",
    "        \n",
    "        if hasattr(self, 'one_hot_node_embeddings_encoder_'):\n",
    "            X = self.estimator.apply(X)\n",
    "            X = self.one_hot_node_embeddings_encoder_.transform(X)\n",
    "        \n",
    "        else:\n",
    "            X = self.estimator.apply(X)\n",
    "            self.one_hot_node_embeddings_encoder_ = OneHotEncoder().fit(X)\n",
    "            X = self.one_hot_node_embeddings_encoder_.transform(X)\n",
    "            \n",
    "        return X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ForestKernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CategoricalKernel(EstimatorKernel):    \n",
    "    '''\n",
    "    Linear model kernel for high cardinality categorical variables.\n",
    "    kernel space is defined by liner model coefficients indexed by the nonzero elements\n",
    "    of X\n",
    "    '''\n",
    "    def __init__(self, estimator, norm = 'l2', use_encoder = False):\n",
    "        self.use_encoder = use_encoder\n",
    "        super().__init__(estimator, norm)\n",
    "        return    \n",
    "    \n",
    "    def fit(self, X, y = None, save_values = None, **kwargs):\n",
    "        \n",
    "        if self.use_encoder:\n",
    "            self.estimator = make_pipeline(RobustEncoder(), self.estimator)\n",
    "                \n",
    "        return super().fit(X, y, save_values, **kwargs)\n",
    "            \n",
    "    \n",
    "    def project(self, X):\n",
    "        '''\n",
    "        multiplies sparse vector to its coef_ s from linear model.\n",
    "        if multiclass classification, the number of final features will be\n",
    "        n*original_n_features_before_one_hot_encoding\n",
    "        '''\n",
    "        \n",
    "        if self.use_encoder:\n",
    "            coefs = self.estimator[-1].coef_\n",
    "            X = self.estimator[0].transform(X)\n",
    "        else:            \n",
    "            coefs = self.estimator.coef_\n",
    "                \n",
    "        if len(coefs.shape) == 1:\n",
    "            coefs = coefs.reshape(1,-1)\n",
    "        \n",
    "        embeddings = []\n",
    "        for dim in range(coefs.shape[0]):\n",
    "            #assumes all rows have the same ammount of nonzero elements\n",
    "            dim_embeddings = coefs[dim, X.nonzero()[1]].reshape(X.shape[0], len(X[0].data))                        \n",
    "            embeddings.append(dim_embeddings)\n",
    "                \n",
    "        return np.hstack(embeddings)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000, 35), (1000, 14))"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_features = 7\n",
    "cardinality_per_feature = 1000\n",
    "n_classes = 5\n",
    "n_reg_dims = 2\n",
    "\n",
    "X = np.random.randint(0,cardinality_per_feature,(1000,n_features))\n",
    "\n",
    "y_class = np.random.randint(0,n_classes, 1000)\n",
    "y_reg = np.random.randn(1000,n_reg_dims)\n",
    "\n",
    "kernel_class = CategoricalKernel(LogisticRegression(), use_encoder = True)\n",
    "kernel_reg = CategoricalKernel(LinearRegression(), use_encoder = True)\n",
    "kernel_class.fit(X,y_class)\n",
    "kernel_reg.fit(X,y_reg)\n",
    "\n",
    "kernel_class.project(X).shape, kernel_reg.project(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted kernel.ipynb.\n"
     ]
    }
   ],
   "source": [
    "#hide\n",
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
