# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks_dev/kernel.ipynb (unless otherwise specified).

__all__ = ['make_batches', 'sparsify', 'sim_matrix_to_idx_and_score', 'cosine_similarity', 'cosine_distance', 'hstack',
           'vstack', 'stack', 'RobustEncoder', 'EstimatorKernel', 'ForestKernel', 'CategoricalKernel']

# Cell
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import normalize, OneHotEncoder, OrdinalEncoder
from sklearn.ensemble import RandomTreesEmbedding, RandomForestClassifier
from sklearn.pipeline import make_pipeline

from scipy import sparse
import numpy as np

from sparse_dot_topn import awesome_cossim_topn


# Cell
def make_batches(arr, batch_size = 100):
    '''make batches for batch query'''
    #lst = [i for i in arr]

    if arr.shape[0] < batch_size:
        batches = [arr]
    else:
        n_bs = arr.shape[0] // batch_size
        last_batch = arr.shape[0] - batch_size * n_bs
        batches = []
        i = 0
        for i in range(n_bs):
            yield arr[i * batch_size:(i + 1) * batch_size]

        if last_batch:
            yield arr[(i + 1) * batch_size:]


# Cell
def sparsify(*arrs):
    '''
    makes input arrs sparse
    '''
    arrs = list(arrs)
    for i in range(len(arrs)):
        if not sparse.issparse(arrs[i]):
            arrs[i] = sparse.csr_matrix(arrs[i])

    return arrs

def sim_matrix_to_idx_and_score(sim_matrix):
    '''
    returns list of indexes (col index of row vector) and scores (similarity value) for each row, given a similarity matrix
    '''
    scores = []
    idxs = []
    for row in sim_matrix:
        idxs.append(row.nonzero()[-1])
        scores.append(row.data)

    return idxs, scores

def cosine_similarity(A, B, topn = 30, remove_diagonal = False, **kwargs):

    A,B = sparsify(A,B)
    A = normalize(A, norm  = 'l2').astype(np.float64)
    B = normalize(B, norm  = 'l2').astype(np.float64)
    dot = awesome_cossim_topn(A, B.T, ntop = topn, **kwargs)

    if remove_diagonal:
        dot.setdiag(0)
        dot.eliminate_zeros()

    return dot


def cosine_distance(A, B, topn = 30, remove_diagonal = False, **kwargs):

    #calculate sim
    dist = cosine_similarity(A, B, topn, remove_diagonal, **kwargs)
    #calculate distance
    dist.data = 1 - dist.data
    return dist

# Cell
def _robust_stack(blocks, stack_method = 'stack', **kwargs):

    if any(sparse.issparse(i) for i in blocks):
        stacked = getattr(sparse, stack_method)(blocks, **kwargs)
    else:
        stacked = getattr(np, stack_method)(blocks, **kwargs)
    return stacked

def hstack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'hstack', **kwargs)

def vstack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'vstack', **kwargs)

def stack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'stack', **kwargs)


class RobustEncoder(BaseEstimator, TransformerMixin):

    def __init__(self,):
        '''
        A robust one hot encoder. Always return the same amount of nonzero value sin each transformed row.
        Has columns for unknown values
        '''
        return

    def fit(self, X, y = None, **kwawrgs):
        self.ordinalencoder_ = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1).fit(X)

        X = self.ordinalencoder_.transform(X)

        categories = [np.arange(-1, len(cats)) for cats in self.ordinalencoder_.categories_]
        self.onehotencoder_ = OneHotEncoder(categories = categories).fit(X)
        return self

    def transform(self, X, **kwargs):
        X = self.ordinalencoder_.transform(X)
        return self.onehotencoder_.transform(X)

# Cell
class EstimatorKernel(BaseEstimator, TransformerMixin):
    '''
    creates a kernel with some specified estimator.
    projection method will be performed according to projection_method.
    projection method can be a string refering to estimators method used to project,
    or a callable, that receives the estimator and X (vector to be projected) as the inputs.
    should return the projections of X according to estimator.
    norm will normalize vectors in matrices prior to applying dot products.
    '''
    def __init__(self, estimator, projection_method = 'predict_proba', norm = 'l2'):
        '''
        creates a kernel with some specified estimator.
        projection method will be performed according to projection_method.
        projection method can be a string refering to estimators method used to project,
        or a callable, that receives the estimator and X (vector to be projected) as the inputs.
        should return the projections of X according to estimator.
        norm will normalize vectors in matrices prior to applying dot products.
        '''
        self.estimator = estimator
        self.projection_method = projection_method
        self.norm = norm

    def transform(self, X):
        '''
        projects X into new space, according to projection_method
        '''
        if callable(self.projection_method):
            return self.projection_method(self.estimator, X)
        else:
            return getattr(self.estimator, self.projection_method)(X)

    def fit(self, X, y = None, save_values = None, **kwargs):
        '''
        X is the feature space,
        y is used only for supervised Kernels
        save_values are values associated with each "Embeding". During transform,
        the values of saved_values are retrieved according to indexes returned by Nearest Neighbor query
        '''
        if not save_values is None:
            if not len(save_values) == len(X):
                raise IndexError(f'X and save_values must have the same shape along the first dimension. Got {X.shape} and {save_values.shape}')

        self.estimator.fit(X, y, **kwargs)

        self.train_projection_space_ = self.transform(X) #saves projection space of X in train
        self.train_projection_values_ = save_values #saves values to be retrieved by some query
        return self

    def similarity_matrix(self, A = None, B = None,  topn = 30, remove_diagonal = False, lower_bound = 0.0, metric = 'cosine'):
        '''
        reeturns pariwise_similarity of X and self.train_projection_space_
        if X is None, returns pariwise similarity of self.train_projection_space_ with itself
        '''

        METRICS = {
            'cosine':cosine_similarity,
        }


        if A is None:
            A = self.train_projection_space_
        else:
            #transform to space
            A = self.transform(A)

        if B is None:
            B = self.train_projection_space_
        else:
            #transform to space
            B = self.transform(B)

        try:
            return METRICS[metric.lower()](
                #normalize first
                normalize(A, norm = self.norm, axis = 0),
                normalize(B, norm = self.norm, axis = 0),
                topn,
                remove_diagonal,
                lower_bound = lower_bound
            )
        except KeyError:
            raise AttributeError(f'metric should be one of {list(METRICS)}, got {metric}')

    def similarity_idxs(self, A = None, B = None, topn = 30, remove_diagonal = False, lower_bound = 0.0, alpha = None, metric = 'cosine'):
        '''
        performs dot product based similarity of normalized X versus normalized self.train_projection_space_.
        if X is None, returns similarity of self.train_projection_space_ within itself

        returns two lists, one of indexes and other of scores, the indexes refer to self.train_projecetion_space_ rows

        alpha is a concentration factor, such that similarity_values = similarity_values**alpha
        alpha > 1 implies spreading points appart
        0 < alpha < 1 implies bringing points closer together
        '''

        sim_matrix = self.similarity_matrix(A, B, topn, remove_diagonal, lower_bound, metric)
        idxs, sim = sim_matrix_to_idx_and_score(sim_matrix)

        #use alpha concentration factor
        if not alpha is None:
            sim = [s**alpha for s in sim]

        return idxs, sim

    def similarity(self, A = None, B = None, topn = 30, remove_diagonal = False, lower_bound = 0.0, alpha = None, metric = 'cosine'):
        '''
        same as similarity, but instead of returning indexes, returns values in self.train_projection_values_
        '''
        idxs, sim = self.similarity_idxs(A, B, topn, remove_diagonal, lower_bound, alpha, metric)
        values = [self.train_projection_values_[idx] for idx in idxs]
        return values, sim

    def update_space(self, X, save_values):
        '''
        updates self.train_projection_space_ and self.train_projection_values_ with new data.
        new values are found running self.transform on X
        '''
        X = self.transform(X)

        self.train_projection_space_ = vstack([self.train_projection_space_, X])
        self.train_projection_values_ = vstack([self.train_projection_values_, save_values])
        return self

# Cell
class ForestKernel(EstimatorKernel):
    '''
    A Space tranformation performed based on Forest transformations.
    Can be supervised or not (CARTs, RandomTreeEmbeddings, Boosted trees...)

    the transformed space can be defined and the `decision_path` space or `terminal_nodes`
    space.
    '''

    def __init__(self, estimator, embedding_space = 'decision_path'):
        self.embedding_space = embedding_space
        super().__init__(estimator, projection_method = None, norm='l2')
        return

    def transform(self, X):

        if self.embedding_space == 'decision_path':

            X, n_nodes_ptr = self.estimator.decision_path(X)

        elif self.embedding_space == 'terminal_nodes':

            if hasattr(self, 'one_hot_node_embeddings_encoder_'):
                X = self.estimator.apply(X)
                X = self.one_hot_node_embeddings_encoder_.transform(X)

            else:
                X = self.estimator.apply(X)
                self.one_hot_node_embeddings_encoder_ = OneHotEncoder().fit(X)
                X = self.one_hot_node_embeddings_encoder_.transform(X)

        else:
            raise ValueError(f'embedding_space should be one of ["decision_path","terminal_nodes"], got {self.embedding_space}')

        return X

# Cell
class CategoricalKernel(EstimatorKernel):
    '''
    Linear model kernel for high cardinality categorical variables.
    kernel space is defined by liner model coefficients indexed by the nonzero elements
    of X
    '''
    def __init__(self, estimator, norm = 'l2', encode = False):
        self.encode = encode
        super().__init__(estimator, norm)
        return

    def fit(self, X, y = None, save_values = None, **kwargs):

        if self.encode:
            self.estimator = make_pipeline(RobustEncoder(), self.estimator)

        return super().fit(X, y, save_values, **kwargs)

    def transform(self, X):
        '''
        multiplies sparse vector to its coef_ s from linear model.
        if multiclass classification, the number of final features will be
        n*original_n_features_before_one_hot_encoding
        '''

        if self.encode:
            coefs = self.estimator[-1].coef_
            X = self.estimator[0].transform(X)
        else:
            coefs = self.estimator.coef_

        #create attr if it does now exist yet:
        #this line is supposed to run only during fit call
        if not hasattr(self,'dim_embeddings_'):
            self.dim_embeddings_ = len(X[0].data)

        if len(coefs.shape) == 1:
            coefs = coefs.reshape(1,-1)

        embeddings = []
        for dim in range(coefs.shape[0]):
            #assumes all rows have the same ammount of nonzero elements
            dim_embeddings = coefs[dim, X.nonzero()[1]].reshape(X.shape[0], self.dim_embeddings_)
            embeddings.append(dim_embeddings)

        return hstack(embeddings)
