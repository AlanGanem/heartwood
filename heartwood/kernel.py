# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks_dev/kernel.ipynb (unless otherwise specified).

__all__ = ['EstimatorKernel', 'SparseForestKernel', 'DenseForestKernel', 'CategoricalLinearKernel',
           'ClassificationLinearBottleneck', 'RegressionLinearBottleneck', 'MLPKernel', 'BOWKernel']

# Cell
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import normalize, OneHotEncoder, OrdinalEncoder
from sklearn.ensemble import RandomTreesEmbedding, RandomForestClassifier, RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.neighbors import NearestNeighbors
from sklearn.decomposition import TruncatedSVD

from scipy import sparse
import numpy as np

from .neighbors import

# Cell
class EstimatorKernel(BaseEstimator, TransformerMixin):
    '''
    creates a kernel with some specified estimator.
    projection method will be performed according to projection_method.
    projection method can be a string refering to estimators method used to project,
    or a callable, that receives the estimator and X (vector to be projected) as the inputs.
    should return the projections of X according to estimator.
    norm will normalize vectors in matrices prior to applying dot products.
    '''
    def __init__(self, estimator, projection_method = 'predict_proba', norm = 'l2'):
        '''
        creates a kernel with some specified estimator.
        projection method will be performed according to projection_method.
        projection method can be a string refering to estimators method used to project,
        or a callable, that receives the estimator and X (vector to be projected) as the inputs.
        should return the projections of X according to estimator.
        norm will normalize vectors in matrices prior to applying dot products.
        '''
        self.estimator = estimator
        self.projection_method = projection_method
        self.norm = norm

    def __getattr__(self, attr):
        '''
        Allows accessing self.estimator attributes if not found in first object level
        '''
        return getattr(self.estimator, attr)

    def transform(self, X):
        '''
        projects X into new space, according to projection_method
        '''
        if callable(self.projection_method):
            return self.projection_method(self.estimator, X)
        else:
            return getattr(self.estimator, self.projection_method)(X)

    def fit(self, X, y = None, save_values = None, **kwargs):
        '''
        X is the feature space,
        y is used only for supervised Kernels
        save_values are values associated with each "Embeding". During transform,
        the values of saved_values are retrieved according to indexes returned by Nearest Neighbor query
        '''
        if not save_values is None:
            if not len(save_values) == len(X):
                raise IndexError(f'X and save_values must have the same shape along the first dimension. Got {X.shape} and {save_values.shape}')

        self.estimator.fit(X, y, **kwargs)

        self.train_projection_space_ = self.transform(X) #saves projection space of X in train
        self.train_projection_values_ = save_values #saves values to be retrieved by some query
        return self

    def similarity_matrix(self, A = None, B = None,  topn = 30, remove_diagonal = False, lower_bound = 0.0, metric = 'cosine', alpha = None):
        '''
        reeturns pariwise_similarity of X and self.train_projection_space_
        if X is None, returns pariwise similarity of self.train_projection_space_ with itself
        '''

        METRICS = {
            'cosine':cosine_similarity,
        }


        if A is None:
            A = self.train_projection_space_
        else:
            #transform to space
            A = self.transform(A)

        if B is None:
            B = self.train_projection_space_
        else:
            #transform to space
            B = self.transform(B)

        try:
            sim_matrix = METRICS[metric.lower()](
                #normalize first
                normalize(A, norm = self.norm, axis = 0),
                normalize(B, norm = self.norm, axis = 0),
                topn,
                remove_diagonal,
                lower_bound = lower_bound
            )

            #apply alpha concentration factor
            if not alpha is None:
                sim_matrix.data = sim_matrix.data**alpha

            return sim_matrix

        except KeyError:
            raise AttributeError(f'metric should be one of {list(METRICS)}, got {metric}')

    def similarity_idxs(self, A = None, B = None, topn = 30, remove_diagonal = False, lower_bound = 0.0, alpha = None, metric = 'cosine'):
        '''
        performs dot product based similarity of normalized X versus normalized self.train_projection_space_.
        if X is None, returns similarity of self.train_projection_space_ within itself

        returns two lists, one of indexes and other of scores, the indexes refer to self.train_projecetion_space_ rows

        alpha is a concentration factor, such that similarity_values = similarity_values**alpha
        alpha > 1 implies spreading points appart
        0 < alpha < 1 implies bringing points closer together
        '''

        sim_matrix = self.similarity_matrix(A, B, topn, remove_diagonal, lower_bound, metric, alpha)
        idxs, sim = sim_matrix_to_idx_and_score(sim_matrix)

        return idxs, sim

    def similarity(self, A = None, B = None, topn = 30, remove_diagonal = False, lower_bound = 0.0, alpha = None, metric = 'cosine'):
        '''
        same as similarity, but instead of returning indexes, returns values in self.train_projection_values_
        '''
        idxs, sim = self.similarity_idxs(A, B, topn, remove_diagonal, lower_bound, alpha, metric)
        values = [self.train_projection_values_[idx] for idx in idxs]
        return values, sim

    def update_space(self, X, save_values):
        '''
        updates self.train_projection_space_ and self.train_projection_values_ with new data.
        new values are found running self.transform on X
        '''
        X = self.transform(X)

        self.train_projection_space_ = vstack([self.train_projection_space_, X])
        self.train_projection_values_ = vstack([self.train_projection_values_, save_values])
        return self

# Cell
class SparseForestKernel(EstimatorKernel):
    '''
    A Space tranformation performed based on Forest transformations.
    Can be supervised or not (CARTs, RandomTreeEmbeddings, Boosted trees...)

    the embedding_space is sparse and can be defined as the `decision_path` space or `terminal_nodes`
    space.
    '''

    def __init__(self, estimator, embedding_space = 'decision_path'):
        self.embedding_space = embedding_space
        super().__init__(estimator, projection_method = None, norm='l2')
        return

    def transform(self, X):

        if self.embedding_space == 'decision_path':

            X, n_nodes_ptr = self.estimator.decision_path(X)

        elif self.embedding_space == 'terminal_nodes':

            if hasattr(self, 'one_hot_node_embeddings_encoder_'):
                X = self.estimator.apply(X)
                X = self.one_hot_node_embeddings_encoder_.transform(X)

            else:
                X = self.estimator.apply(X)
                self.one_hot_node_embeddings_encoder_ = OneHotEncoder().fit(X)
                X = self.one_hot_node_embeddings_encoder_.transform(X)

        else:
            raise ValueError(f'embedding_space should be one of ["decision_path","terminal_nodes"], got {self.embedding_space}')

        return X

# Cell
class DenseForestKernel(EstimatorKernel):
    '''
    combines SparseForestTree and a some densifier (LinearBottleneck or CategoricalLinearKernel) to compress sparse
    representations to a dense one
    '''
    def __init__(self, forest_estimator, n_components, densifier = 'categorical_linear', embedding_space = 'decision_path'):

        self.forest_estimator = forest_estimator
        self.densifier = densifier
        self.n_components = n_components
        self.embedding_space = embedding_space
        return

    def _set_full_estimator_pipeline(self, ):

        AVALIBLE_DENSIFIERS = ['categorical_linear','linear_bottleneck','k_means_embeddings']

        is_regression = False
        is_classification = False
        is_transformation = False

        #check inference type of estimator
        if hasattr(self.forest_estimator, 'predict_proba'):
            is_classification = True
        elif hasattr(self.forest_estimator, 'predict'):
            is_regression = True
        else:
            is_transformation = True

        if isinstance(self.densifier, str):
            if not self.densifier.lower() in AVALIBLE_DENSIFIERS:
                raise ValueError(f'If str, densifier should be one of {AVALIBLE_DENSIFIERS}, got {self.densifier}')

        if is_transformation:
            if isinstance(self.densifier, str):
                raise TypeError(f'if forest_estimator is a transformer (not classifier nor regressor), densifier should be an instance of BaseEstimator, not str')

        if is_classification:
            if isinstance(self.densifier, str):
                if self.densifier.lower() == 'categorical_linear':
                    self.densifier = CategoricalLinearKernel(
                        SGDClassifier(loss = 'log'), encode = True, n_components=self.n_components
                    )
                elif self.densifier.lower() == 'linear_bottleneck':
                    self.densifier = ClassificationLinearBottleneck(n_components=self.n_components)












    def fit(self, X, save):




# Cell
class CategoricalLinearKernel(EstimatorKernel):
    '''
    Linear model kernel recommended for high cardinality one hot encoded categorical variables.
    kernel space is defined by liner model coefficients indexed by the nonzero elements
    of X

    If encode is set to true, a customized onehotencoder will encode the categorical input.

    This kernel will only work if the output of the one hot encoded vectors  have always the same number
    of nonzero elements (equal to the number of categorical features). Thus, its recomended to use the default
    encoder, because it asserts this condition is met during one hot encoding
    '''
    def __init__(self, estimator, norm = 'l2', encode = False, n_components = None, **pcakwargs):
        self.encode = encode
        self.n_components = n_components
        self.pcakwargs = pcakwargs
        super().__init__(estimator, norm)
        return

    def fit(self, X, y = None, save_values = None, **kwargs):

        if self.encode:
            if not self.n_components is None:
                self.estimator = make_pipeline(RobustEncoder(), self.estiamtor, PCA(self.n_components, **self.pcakwargs))
            else:
                self.estimator = make_pipeline(RobustEncoder(), self.estimator)
        else:
            if not self.n_components is None:
                self.estimator = make_pipeline(self.estiamtor, PCA(self.n_components, **self.pcakwargs))
            else:
                pass

        return super().fit(X, y, save_values, **kwargs)

    def transform(self, X):
        '''
        multiplies sparse vector to its coef_ s from linear model.
        if multiclass classification, the number of final features will be
        n*original_n_features_before_one_hot_encoding
        '''

        if self.encode:
            coefs = self.estimator[-1].coef_
            X = self.estimator[0].transform(X)
        else:
            coefs = self.estimator.coef_

        #create attr if it does now exist yet:
        #this line is supposed to run only during fit call
        if not hasattr(self,'dim_embeddings_'):
            self.dim_embeddings_ = len(X[0].data)

        if len(coefs.shape) == 1:
            coefs = coefs.reshape(1,-1)

        embeddings = []
        for dim in range(coefs.shape[0]):
            #assumes all rows have the same ammount of nonzero elements
            dim_embeddings = coefs[dim, X.nonzero()[1]].reshape(X.shape[0], self.dim_embeddings_)
            embeddings.append(dim_embeddings)

        return hstack(embeddings)


# Cell
class ClassificationLinearBottleneck(MLPClassifier):
    '''
    Linear boottleneck of a classification task, usefull for dimensionality reduction
    or densification of sparse representations.
    '''
    def __init__(
        self,
        n_components = 2,
        solver='adam',
        alpha=0.0001,
        batch_size='auto',
        learning_rate='constant',
        learning_rate_init=0.001,
        power_t=0.5,
        max_iter=200,
        shuffle=True,
        random_state=None,
        tol=0.0001,
        verbose=False,
        warm_start=False,
        momentum=0.9,
        nesterovs_momentum=True,
        early_stopping=False,
        validation_fraction=0.1,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-08,
        n_iter_no_change=10,
        max_fun=15000,
    ):

        #set attributes, some will be overriden in super().__init__ call
        self.solver = solver
        self.alpha = alpha
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.learning_rate_init = learning_rate_init
        self.power_t = power_t
        self.max_iter = max_iter
        self.shuffle = shuffle
        self.random_state = random_state
        self.tol = tol
        self.verbose = verbose
        self.warm_start = warm_start
        self.momentum = momentum
        self.nesterovs_momentum = nesterovs_momentum
        self.early_stopping = early_stopping
        self.validation_fraction = validation_fraction
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        self.n_iter_no_change = n_iter_no_change
        self.max_fun = max_fun

        super().__init__(hidden_layer_sizes = (n_components,), activation = 'identity', **self.__dict__)
        self.n_components = n_components
        return

    def transform(self, X, **kwargs):
        '''
        projects inputs to have size (n_samples, n_components)
        '''
        return _get_sklearn_mlp_activations(self, X)


class RegressionLinearBottleneck(MLPRegressor):
    '''
    Linear boottleneck of a classification task, usefull for dimensionality reduction
    or densification of sparse representations.
    '''
    def __init__(
        self,
        n_components = 2,
        solver='adam',
        alpha=0.0001,
        batch_size='auto',
        learning_rate='constant',
        learning_rate_init=0.001,
        power_t=0.5,
        max_iter=200,
        shuffle=True,
        random_state=None,
        tol=0.0001,
        verbose=False,
        warm_start=False,
        momentum=0.9,
        nesterovs_momentum=True,
        early_stopping=False,
        validation_fraction=0.1,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-08,
        n_iter_no_change=10,
        max_fun=15000,
    ):

        #set attributes, some will be overriden in super().__init__ call
        self.solver = solver
        self.alpha = alpha
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.learning_rate_init = learning_rate_init
        self.power_t = power_t
        self.max_iter = max_iter
        self.shuffle = shuffle
        self.random_state = random_state
        self.tol = tol
        self.verbose = verbose
        self.warm_start = warm_start
        self.momentum = momentum
        self.nesterovs_momentum = nesterovs_momentum
        self.early_stopping = early_stopping
        self.validation_fraction = validation_fraction
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        self.n_iter_no_change = n_iter_no_change
        self.max_fun = max_fun

        super().__init__(hidden_layer_sizes = (n_components,), activation = 'identity', **self.__dict__)
        self.n_components = n_components
        return

    def transform(self, X, **kwargs):
        '''
        projects inputs to have size (n_samples, n_components)
        '''
        return _get_sklearn_mlp_activations(self, X)

# Cell
def _get_sklearn_mlp_activations(self, X, output_layer = -2):
    hidden_layer_sizes = self.hidden_layer_sizes
    if not hasattr(hidden_layer_sizes, "__iter__"):
        hidden_layer_sizes = [hidden_layer_sizes]
    hidden_layer_sizes = list(hidden_layer_sizes)
    layer_units = [X.shape[1]] + hidden_layer_sizes + \
        [self.n_outputs_]
    activations = [X]
    for i in range(self.n_layers_ - 1):
        activations.append(np.empty((X.shape[0],
                                     layer_units[i + 1])))
    self._forward_pass(activations)
    return activations[output_layer]


class MLPKernel(EstimatorKernel):

    '''
    returns the output of last hidden layer (before softmax/linear layer)
    as space projection.

    Recomended for dimensionality reduction and Context specific bag of words task
    '''
    def __init__(self, estimator, output_layer = -2, norm='l2'):

        self.projection_method = partial(_get_sklearn_mlp_activations, output_layer = output_layer)
        self.estimator = estimator
        self.output_layer = output_layer
        self.norm = norm
        return



# Cell
class BOWKernel(MLPKernel):
    '''
    `MLPKernel` Alias, intended for Bag Of Words application.
    Generates supervised embeddings (context specific embeddings)
    '''
    pass