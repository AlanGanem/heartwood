# AUTOGENERATED! DO NOT EDIT! File to edit: notebooks_dev/kernel.ipynb (unless otherwise specified).

__all__ = ['sigmoid', 'make_bimodal_assymetric_regression', 'sparsify', 'hstack', 'vstack', 'stack', 'RobustEncoder',
           'EstimatorKernel', 'JaccardForestKernel', 'CategoricalLinearKernel', 'ClassificationLinearBottleneck',
           'RegressionLinearBottleneck', 'MLPKernel', 'BOWKernel', 'DiscretizedTargetKernel']

# Cell
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.preprocessing import normalize, OneHotEncoder, OrdinalEncoder, KBinsDiscretizer
from sklearn.pipeline import make_pipeline
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.linear_model import SGDClassifier, SGDRegressor
from sklearn.decomposition import TruncatedSVD

from scipy import sparse
import numpy as np

from nmslearn.neighbors import FastJaccardNN, FastKLDivNN, FastL2NN
from .utils import hstack, RobustEncoder

# Cell
from sklearn.datasets import make_regression

def sigmoid(x):
    return 1/(1+np.exp(x))

def make_bimodal_assymetric_regression(
    n_samples=100000,
    bimodal_factor_weight = 2,
    n_features=15,
    n_informative=6,
    n_targets=2,
    bias=500,
    effective_rank=None,
    tail_strength=10,
    noise=150,
    shuffle=True,
    coef=False,
    random_state=None
):

    X,y = make_regression(
        n_samples=n_samples,
        n_features=n_features,
        n_informative=n_informative,
        n_targets=n_targets,
        bias=bias,
        effective_rank=effective_rank,
        tail_strength=tail_strength,
        noise=noise,
        shuffle=shuffle,
        coef=coef,
        random_state=random_state
    )


    #make one of X[1] feature mode weightening
    bimodal_factors = (sigmoid(bimodal_factor_weight*X[:,-1]) > np.random.random(size = X.shape[0])).astype(int)
    bimodal_factors[bimodal_factors == 0] = -1
    bimodal_factors = bimodal_factors.reshape(-1,1)

    y = bimodal_factors*y

    return X,y

# Cell
#export
def sparsify(*arrs):
    '''
    makes input arrs sparse
    '''
    arrs = list(arrs)
    for i in range(len(arrs)):
        if not sparse.issparse(arrs[i]):
            arrs[i] = sparse.csr_matrix(arrs[i])

    return arrs

def _robust_stack(blocks, stack_method = 'stack', **kwargs):

    if any(sparse.issparse(i) for i in blocks):
        stacked = getattr(sparse, stack_method)(blocks, **kwargs)
    else:
        stacked = getattr(np, stack_method)(blocks, **kwargs)
    return stacked

def hstack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'hstack', **kwargs)

def vstack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'vstack', **kwargs)

def stack(blocks, **kwargs):
    return _robust_stack(blocks, stack_method = 'stack', **kwargs)


class RobustEncoder(BaseEstimator, TransformerMixin):

    def __init__(self,):
        '''
        A robust one hot encoder. Always return the same amount of nonzero value sin each transformed row.
        Has columns for unknown values
        '''
        return

    def fit(self, X, y = None, **kwawrgs):
        self.ordinalencoder_ = OrdinalEncoder(handle_unknown = 'use_encoded_value', unknown_value = -1).fit(X)

        X = self.ordinalencoder_.transform(X)

        categories = [np.arange(-1, len(cats)) for cats in self.ordinalencoder_.categories_]
        self.onehotencoder_ = OneHotEncoder(categories = categories).fit(X)
        return self

    def transform(self, X, **kwargs):
        X = self.ordinalencoder_.transform(X)
        return self.onehotencoder_.transform(X)

# Cell
class EstimatorKernel(BaseEstimator, TransformerMixin):
    '''
    creates a kernel with some specified estimator.
    projection method will be performed according to projection_method.
    projection method can be a string refering to estimators method used to project,
    or a callable, that receives the estimator and X (vector to be projected) as the inputs.
    should return the projections of X according to estimator.
    norm will normalize vectors in matrices prior to applying dot products.
    '''
    def __init__(self, estimator, projection_method, nearest_neighbors_estimator, fit_neighbors_index = True, n_neighbors = 30):
        '''
        creates a kernel with some specified estimator.
        projection method will be performed according to projection_method.
        projection method can be a string refering to estimators method used to project,
        or a callable, that receives the estimator and X (vector to be projected) as the inputs.
        should return the projections of X according to estimator.
        norm will normalize vectors in matrices prior to applying dot products.
        '''
        self.estimator = estimator
        self.projection_method = projection_method
        self.nearest_neighbors_estimator = nearest_neighbors_estimator
        self.n_neighbors = n_neighbors
        self.fit_neighbors_index = fit_neighbors_index


    def __getattr__(self, attr):
        '''
        Allows accessing self.estimator attributes if not found in first object level
        '''
        return getattr(self.estimator, attr)

    def transform(self, X):
        '''
        projects X into new space, according to projection_method
        '''

        if callable(self.projection_method):
            return self.projection_method(self.estimator, X)
        else:
            return getattr(self.estimator, self.projection_method)(X)

    def fit(self, X, y = None, save_values = None, **kwargs):
        '''
        X is the feature space,
        y is used only for supervised Kernels
        save_values are values associated with each "Embeding". During transform,
        the values of saved_values are retrieved according to indexes returned by Nearest Neighbor query
        '''
        if not save_values is None:
            if not len(save_values) == len(X):
                raise IndexError(f'X and save_values must have the same shape along the first dimension. Got {X.shape} and {save_values.shape}')

        self.estimator.fit(X, y, **kwargs)

        if self.fit_neighbors_index:
            #make space transformation
            train_projection_space_ = self.transform(X) #saves projection space of X in train

            if save_values is None:
                save_values = np.empty((X.shape[0], 0)) #empty array

            #fit index
            self.nearest_neighbors_estimator.fit(train_projection_space_)

            #save states
            #self.train_projection_space_ = train_projection_space_ #is it really necessary to save this? possibly yieds memory overhead
            self.train_projection_values_ = save_values

        return self

    def dist_matrix(self, A, B,  n_neighbors = 30, remove_diagonal = False):
        '''
        reeturns pariwise_dist of X and self.train_projection_space_
        if X is None, returns pariwise similarity of self.train_projection_space_ with itself
        '''
        raise NotImplementedError


    def kneighbors(self, X = None, n_neighbors = None, return_distance = True):
        '''
        runs nearest neighbor search in the transformed space index
        '''

        if not self.fit_neighbors_index:
            raise AattributeError('This method is only available when fit_neighbors_index is set to True in the constructor')

        if n_neighbors is None:
            n_neighbors = self.n_neighbors

        #make space transformation
        X = self.transform(X)
        #query in index
        result = self.nearest_neighbors_estimator.kneighbors(X, n_neighbors = n_neighbors, return_distance = return_distance)

        return result #dist, idxs

    def query(self, X = None, n_neighbors = None):
        '''
        same as kneighbors, but instead of returning indexes, returns values in self.train_projection_values_
        '''
        if not self.fit_neighbors_index:
            raise AattributeError('This method is only available when fit_neighbors_index is set to True in the constructor')

        dist, idxs = self.kneighbors(X, n_neighbors, return_distance = True)

        if hasattr(self.train_projection_values_, 'iloc'):
            values = [self.train_projection_values_.iloc[idx] for idx in idxs]

        else:
            values = [self.train_projection_values_[idx] for idx in idxs]

        return values, dist

    def update_space(self, X, save_values = None):
        '''
        updates self.train_projection_space_ and self.train_projection_values_ with new data.
        new values are found running self.transform on X
        '''
        if not self.fit_neighbors_index:
            raise AattributeError('This method is only available when fit_neighbors_index is set to True in the constructor')

        X = self.transform(X)

        self.train_projection_space_ = vstack([self.train_projection_space_, X])

        if save_values is None:
            save_values = np.empty((X.shape[0], self.train_projection_values_.shape[-1])) #empty array


        #refit knn index
        self.nearest_neighbors_estimator.fit(self.train_projection_space_)

        self.train_projection_values_ = vstack([self.train_projection_values_, save_values])
        return self

# Cell
class JaccardForestKernel(EstimatorKernel):
    '''
    A Space tranformation performed based on Forest transformations.
    Can be supervised or not (CARTs, RandomTreeEmbeddings, Boosted trees...)

    the embedding_space is sparse and can be defined as the `decision_path` space or `terminal_nodes`
    space.
    '''

    def __init__(
        self,
        estimator,
        n_neighbors = 30,
        fit_neighbors_index = True,
        index_time_params={'M': 30, 'indexThreadQty': 4, 'efConstruction': 100, 'post': 0},
        query_time_params={'efSearch': 100},
        verbose = False
    ):

        #save init params for sklearn consistency
        self.n_neighbors = n_neighbors
        self.index_time_params=index_time_params
        self.query_time_params=query_time_params
        self.verbose=verbose
        self.fit_neighbors_index=fit_neighbors_index

        #instantiate jacard distance nearest neighbor index
        nn_obj = FastJaccardNN(
            n_neighbors = n_neighbors,
            index_time_params=index_time_params,
            query_time_params=query_time_params,
            verbose=verbose,
        )

        super().__init__(
            estimator = estimator,
            projection_method=None,
            nearest_neighbors_estimator=nn_obj,
            fit_neighbors_index=fit_neighbors_index,
            n_neighbors=n_neighbors
        )

        return

    def transform(self, X):

        X = self.estimator.apply(X)
        #handle boosting case
        if len(X.shape) > 2:
            X = X.reshape(X.shape[0], X.shape[1]*X.shape[2])

        if hasattr(self, 'one_hot_node_embeddings_encoder_'):
            X = self.one_hot_node_embeddings_encoder_.transform(X)

        else:
            self.one_hot_node_embeddings_encoder_ = OneHotEncoder().fit(X)
            X = self.one_hot_node_embeddings_encoder_.transform(X)

        return X

# Cell
class CategoricalLinearKernel(EstimatorKernel):
    '''
    Linear model kernel recommended for high cardinality one hot encoded categorical variables.
    kernel space is defined by liner model coefficients indexed by the nonzero elements
    of X

    If encode is set to true, a customized onehotencoder will encode the categorical input.

    This kernel will only work if the output of the one hot encoded vectors  have always the same number
    of nonzero elements (equal to the number of categorical features). Thus, its recomended to use the default
    encoder, because it asserts this condition is met during one hot encoding
    '''

    def __init__(
        self,
        estimator,
        n_neighbors=30,
        fit_neighbors_index = True,
        encode = False,
        n_components = None,
        index_time_params={'M': 30, 'indexThreadQty': 8, 'efConstruction': 100, 'post': 0},
        query_time_params={'efSearch': 100},
        verbose=False,
        **pcakwargs
    ):

        self.encode = encode
        self.n_components = n_components
        self.pcakwargs = pcakwargs
        self.estimator = estimator

        #save init params for sklearn consistency
        self.n_neighbors = n_neighbors
        self.index_time_params=index_time_params
        self.query_time_params=query_time_params
        self.verbose=verbose
        self.fit_neighbors_index=fit_neighbors_index

        #instantiate jacard distance nearest neighbor index
        nn_obj = FastL2NN(
            n_neighbors = n_neighbors,
            index_time_params=index_time_params,
            query_time_params=query_time_params,
            verbose=verbose,
        )

        super().__init__(
            estimator = estimator,
            projection_method=None,
            nearest_neighbors_estimator=nn_obj,
            fit_neighbors_index=fit_neighbors_index,
            n_neighbors=n_neighbors
        )

        return

    def fit(self, X, y = None, save_values = None, **kwargs):

        if self.encode:
            if not self.n_components is None:
                self.estimator = make_pipeline(RobustEncoder(), self.estiamtor, PCA(self.n_components, **self.pcakwargs))
            else:
                self.estimator = make_pipeline(RobustEncoder(), self.estimator)
        else:
            if not self.n_components is None:
                self.estimator = make_pipeline(self.estiamtor, PCA(self.n_components, **self.pcakwargs))
            else:
                pass

        return super().fit(X, y, save_values, **kwargs)

    def transform(self, X):
        '''
        multiplies sparse vector to its coef_ s from linear model.
        if multiclass classification, the number of final features will be
        n*original_n_features_before_one_hot_encoding
        '''

        if self.encode:
            coefs = self.estimator[-1].coef_
            X = self.estimator[0].transform(X)
        else:
            coefs = self.estimator.coef_

        #create attr if it does now exist yet:
        #this line is supposed to run only during fit call
        if not hasattr(self,'dim_embeddings_'):
            self.dim_embeddings_ = len(X[0].data)

        if len(coefs.shape) == 1:
            coefs = coefs.reshape(1,-1)

        embeddings = []
        for dim in range(coefs.shape[0]):
            #assumes all rows have the same ammount of nonzero elements
            dim_embeddings = coefs[dim, X.nonzero()[1]].reshape(X.shape[0], self.dim_embeddings_)
            embeddings.append(dim_embeddings)

        return hstack(embeddings)


# Cell
class ClassificationLinearBottleneck(MLPClassifier):
    '''
    Linear boottleneck of a classification task, usefull for dimensionality reduction
    or densification of sparse representations.
    '''
    def __init__(
        self,
        n_components = 2,
        solver='adam',
        alpha=0.0001,
        batch_size='auto',
        learning_rate='constant',
        learning_rate_init=0.001,
        power_t=0.5,
        max_iter=200,
        shuffle=True,
        random_state=None,
        tol=0.0001,
        verbose=False,
        warm_start=False,
        momentum=0.9,
        nesterovs_momentum=True,
        early_stopping=False,
        validation_fraction=0.1,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-08,
        n_iter_no_change=10,
        max_fun=15000,
    ):

        #set attributes, some will be overriden in super().__init__ call
        self.solver = solver
        self.alpha = alpha
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.learning_rate_init = learning_rate_init
        self.power_t = power_t
        self.max_iter = max_iter
        self.shuffle = shuffle
        self.random_state = random_state
        self.tol = tol
        self.verbose = verbose
        self.warm_start = warm_start
        self.momentum = momentum
        self.nesterovs_momentum = nesterovs_momentum
        self.early_stopping = early_stopping
        self.validation_fraction = validation_fraction
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        self.n_iter_no_change = n_iter_no_change
        self.max_fun = max_fun

        super().__init__(hidden_layer_sizes = (n_components,), activation = 'identity', **self.__dict__)
        self.n_components = n_components
        return

    def transform(self, X, **kwargs):
        '''
        projects inputs to have size (n_samples, n_components)
        '''
        return _get_sklearn_mlp_activations(self, X)


class RegressionLinearBottleneck(MLPRegressor):
    '''
    Linear boottleneck of a classification task, usefull for dimensionality reduction
    or densification of sparse representations.
    '''
    def __init__(
        self,
        n_components = 2,
        solver='adam',
        alpha=0.0001,
        batch_size='auto',
        learning_rate='constant',
        learning_rate_init=0.001,
        power_t=0.5,
        max_iter=200,
        shuffle=True,
        random_state=None,
        tol=0.0001,
        verbose=False,
        warm_start=False,
        momentum=0.9,
        nesterovs_momentum=True,
        early_stopping=False,
        validation_fraction=0.1,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-08,
        n_iter_no_change=10,
        max_fun=15000,
    ):

        #set attributes, some will be overriden in super().__init__ call
        self.solver = solver
        self.alpha = alpha
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.learning_rate_init = learning_rate_init
        self.power_t = power_t
        self.max_iter = max_iter
        self.shuffle = shuffle
        self.random_state = random_state
        self.tol = tol
        self.verbose = verbose
        self.warm_start = warm_start
        self.momentum = momentum
        self.nesterovs_momentum = nesterovs_momentum
        self.early_stopping = early_stopping
        self.validation_fraction = validation_fraction
        self.beta_1 = beta_1
        self.beta_2 = beta_2
        self.epsilon = epsilon
        self.n_iter_no_change = n_iter_no_change
        self.max_fun = max_fun

        super().__init__(hidden_layer_sizes = (n_components,), activation = 'identity', **self.__dict__)
        self.n_components = n_components
        return

    def transform(self, X, **kwargs):
        '''
        projects inputs to have size (n_samples, n_components)
        '''
        return _get_sklearn_mlp_activations(self, X)

# Cell
def _get_sklearn_mlp_activations(self, X, output_layer = -2):
    hidden_layer_sizes = self.hidden_layer_sizes
    if not hasattr(hidden_layer_sizes, "__iter__"):
        hidden_layer_sizes = [hidden_layer_sizes]
    hidden_layer_sizes = list(hidden_layer_sizes)
    layer_units = [X.shape[1]] + hidden_layer_sizes + \
        [self.n_outputs_]
    activations = [X]
    for i in range(self.n_layers_ - 1):
        activations.append(np.empty((X.shape[0],
                                     layer_units[i + 1])))
    self._forward_pass(activations)
    return activations[output_layer]

# Cell
class MLPKernel(EstimatorKernel):

    '''
    returns the output of last hidden layer (before softmax/linear layer)
    as space projection.

    Recomended for dimensionality reduction and Context specific bag of words task
    '''
    def __init__(self, estimator, output_layer = -2, norm='l2'):

        self.projection_method = partial(_get_sklearn_mlp_activations, output_layer = output_layer)
        self.estimator = estimator
        self.output_layer = output_layer
        self.norm = norm
        return



# Cell
class BOWKernel(MLPKernel):
    '''
    `MLPKernel` Alias, intended for Bag Of Words application.
    Generates supervised embeddings (context specific embeddings)
    '''
    pass

# Cell

class DiscretizedTargetKernel(EstimatorKernel):

    def __init__(
        self,
        estimator,
        n_bins=10,
        encode='ordinal',
        strategy='kmeans',
        n_neighbors=30,
        fit_neighbors_index = True,
        index_time_params={'indexThreadQty': 8, 'efConstruction': 100},
        query_time_params={'efSearch': 100},
        verbose=False,
    ):


        #save init params for sklearn consistency
        self.n_neighbors = n_neighbors
        self.index_time_params=index_time_params
        self.query_time_params=query_time_params
        self.verbose=verbose
        self.fit_neighbors_index=fit_neighbors_index
        self.n_bins=n_bins
        self.encode=encode
        self.strategy=strategy

        #instantiate KL Divergence nearest neighbor index
        nn_obj = FastKLDivNN(
            n_neighbors = n_neighbors,
            index_time_params=index_time_params,
            query_time_params=query_time_params,
            verbose=verbose,
        )

        super().__init__(
            estimator = estimator,
            projection_method='predict_proba',
            nearest_neighbors_estimator=nn_obj,
            fit_neighbors_index=fit_neighbors_index,
            n_neighbors=n_neighbors
        )

        return

    def fit(self, X, y = None, save_values = None, **kwargs):

        discretizer = KBinsDiscretizer(
            n_bins = self.n_bins,
            encode = self.encode,
            strategy = self.strategy,

        )

        y = discretizer.fit_transform(y)

        self.discretizer = discretizer
        return super().fit(X, y, save_values, **kwargs)

    def transform(self, X):

        X = self.estimator.predict_proba(X)
        #handle multiclass (multi dimensional joint dist)
        if isinstance(X, list):
            X = hstack(X)
        return X

